{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1 - Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalized form of cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L = \\frac { 1 } { N } \\sum _ { i } L _ { i } \\left( f \\left( x _ { i } , W \\right) , y _ { i } \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L_i = \\sum _ { j \\neq y _ { i } } \\left\\{ \\begin{array} { l } { 0 \\text { if } s _ { y _ { i } } \\geq s _ { j } + 1} \\\\ { s _ { j } - s _ { y _ { i } } + 1 \\text{ otherwise}} \\end{array} \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$j$ is a class\n",
    "\n",
    "$y_i$ is the ground truth (true class) for the i-th training example\n",
    "\n",
    "$s_{y_i}$ is the score of the hypothesis for the ground truth\n",
    "\n",
    "$s_j$ is the score for the j-th class (which happens to *not* be the true class of the i-th example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM algorithm\n",
    "\n",
    "https://www.youtube.com/watch?v=1NxnPkZM9bc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=jc2IthslyzM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*relative* sensitivity over absolute\n",
    "https://www.youtube.com/watch?v=Ilg3gGewQ5U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Maximum A Posteriori\n",
    "$$\\theta _ { M A P } = \\operatorname { argmax } p ( \\theta | D )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understand SVM cost, Softmax loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understand directional derivatives\n",
    "\n",
    "https://math.stackexchange.com/questions/223252/why-is-gradient-the-direction-of-steepest-ascent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most common loss functions in machine learning:\n",
    "\n",
    "https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alttext](img/svm_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understand how error gets propagated to matrix of weight $W$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of hypotheses as templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alttext](img/activation_functions.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
